# -*- coding: utf-8 -*-
"""Recommendation for Pratilipi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3JqENLDyPRSXq3LFVenmEcca-M_RRu5
"""

#!pip install pytorch_lightning
#!unzip ds-assignment.zip -d /content/
# !pip install sktime
# !pip install sktime[extras]

import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
import torch.optim as optim

from torch.autograd import Variable
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
import PratilipiTrainDataset
import NCF
from tqdm.notebook import tqdm



def preprocess_ids(id):
  id=str(id)
  id = id.replace('-','')
  return int(id)



if __name__=='__main__':
    u_id={}
    p_id={}
    np.random.seed(123)

    df = pd.read_csv('ds-assignment/user-interactions.csv')
    df_meta=pd.read_csv('ds-assignment/metadata.csv')
    df.drop('Unnamed: 0',axis=1,inplace=True)


    df['pratilipi_id']=df['pratilipi_id'].apply(preprocess_ids)
    df['user_id']=df['user_id'].apply(preprocess_ids)
    df_meta['author_id']=df_meta['author_id'].apply(preprocess_ids)
    df_meta['pratilipi_id']=df_meta['pratilipi_id'].apply(preprocess_ids)


    for idx,i in enumerate(df['user_id']):
      u_id[i]=idx
    for idx,i in enumerate(df['pratilipi_id']):
      p_id[i]=idx

    df['user_id']=df['user_id'].apply(lambda x:u_id[x])
    df['pratilipi_id']=df['pratilipi_id'].apply(lambda x:p_id[x])
    df['rank_latest'] = df.groupby(['user_id'])['updated_at'].rank(method='first', ascending=False)
    train_df = df[~(df['rank_latest'].isin([1,2,3,4,5,6,7,8,9,10,11,12,13,14]))]
    test_df = df[(df['rank_latest'].isin([1,2,3,4,5,6,7,8,9,10,11,12,13,14]))]



    #len(train_df)/len(df),len(test_df)/len(df)-75-25 split

    # drop columns that we no longer need
    train_df = train_df[['user_id', 'pratilipi_id', 'read_percent']]
    test_df = test_df[['user_id', 'pratilipi_id', 'read_percent']]
    #changing it to just interactions
    train_df.loc[:, 'read_percent'] = 1



    num_users = df['user_id'].max()+1
    num_items = df['pratilipi_id'].max()+1

    all_pratilipiIds = df['pratilipi_id'].unique()

    model = NCF(num_users, num_items, train_df, all_pratilipiIds)

    trainer = pl.Trainer(max_epochs=1, gpus=1,reload_dataloaders_every_n_epochs=True,
                         log_every_n_steps=50)

    trainer.fit(model)

    #Evalutaion for Hit Ratio@10
    # User-item pairs for testing
    test_df=test_df[0:100000]
    test_user_item_set = set(zip(test_df['user_id'], test_df['pratilipi_id']))

    # Dict of all items that are interacted with by each user
    user_interacted_items = df.groupby('user_id')['pratilipi_id'].apply(list).to_dict()

    hits = []
    for (u,i) in tqdm(test_user_item_set):
        interacted_items = user_interacted_items[u]
        not_interacted_items = set(all_pratilipiIds) - set(interacted_items)
        selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))
        test_items = selected_not_interacted + [i]

        predicted_labels = np.squeeze(model(torch.tensor([u]*100),
                                            torch.tensor(test_items)).detach().numpy())

        top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]

        if i in top10_items:
            hits.append(1)
        else:
            hits.append(0)

    print("The Hit Ratio @ 10 is {:.2f}".format(np.average(hits)))

    torch.save(model.state_dict(), 'pratilipi_model.pth')
